{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86655934-6a3a-4084-8f75-608c8a27916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qus 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81b83593-4f0a-4065-8b91-c39de9388a5d",
   "metadata": {},
   "source": [
    "Web Scrapiing is the process of extracting data from websites. it involves automatically fetching web pages , parsing through the HTML or other markup languages, and extracting the desired information for analysis or storage. web scraping is typically performed using specialized software tools,scripts, or programming languages like Python.\n",
    "\n",
    "Here are three common areas where web scraping is used ti gather data:\n",
    "    1. Business Intelligence and Market Research:\n",
    "        Companies use web scraping to gather data on competitors , market trends, pricing information, and consumer sentiment. \n",
    "    2.Content Aggregation:\n",
    "        Many websites aggregate content from various sources on the internet. Web scraping allows them to automate the process of collecting articles, news updates , product listings and other information from multiple websites.\n",
    "    3. Data Science and Research:\n",
    "        Web scrapping is widely used in data science and academic research to collect data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c877ea-e043-4ee3-b5b2-8d720bd3c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qus 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8b355e8-2001-4834-945f-81d6db2db686",
   "metadata": {},
   "source": [
    "There are several methods used for web scraping:\n",
    "    1.Manual Copy-Pasting:\n",
    "        This is the most basic method where users manually copy-paste the required information from web pages into a document or spreadsheet.\n",
    "    2.Browser Extensions:\n",
    "        Browser extensions ike Data Miner, Web Scraper, or Scraper API allow users to extract data from web pages by defining selectors or patterns.\n",
    "    3.Using Built-in Browser Developer Tools:\n",
    "        Modern web browsers come with developer tools that allow users to insepect HTML elements, CSS styles , and network requests.\n",
    "    4. Scraping Libraries/Frameworks:\n",
    "        There are several programming libraries and frameworks designed specifically for web scraping, such as BeautifulSoup (for Python) , Scrapy, Puppeteer (for JavaScript), and Selenium.\n",
    "    5. APIs:\n",
    "        Some websites offer APIs (Application Programming Interfaces) that allow developers to access data in a structured format without the need for scraping .\n",
    "    6. Headless Browsers:\n",
    "        Headless browsers like PhantomJS, Puppeteer , or Selenium WebDriver allow developers to simulate a real browserenvironment without a graphical interface.\n",
    "    7. Proxy Servers:In cases where websites restrict access or impose rate limits on scrapping activities , developers may use proxy servers to rotate IP addresses and distribute requests, preventing their IP from being blocked.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f77f42-ea32-42fd-b16f-0f34d926ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qus 3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "799efaef-e305-41b4-b1f8-c355b0c994ef",
   "metadata": {},
   "source": [
    "Beautiful Soup is a python library that is commonly use3d for web scrapping purposes. It provides tools for parsing HTML and XML documents, navigating the parse tree , and extracting data from web ppages. Beautiful Soup creates a parse tree from the raw HTML and XML source provided , which can then be searched and manipulated to extract specific information.\n",
    "\n",
    "Beautiful Soup is widely used:\n",
    "    1. Easy to use: Beautiful Soup provides a simple and intuitive interface for working with HTML and XML documents.\n",
    "    2. Robust Parsing: It is robust enough to handle poorly formatted or invaliid HTML/XML documents.\n",
    "    3. Powerful Searching : Beautiful Soup allows developers to search for elements inthe parse tree using CSS selectors , XPath expressions , or custom functions.\n",
    "    4. Support for Differewnt Parser Libraries: Beautiful Soup supports various parser libraries, including python's built-in \"html.parser\",\"lxml\",and \"html15lib\".\n",
    "    5. Integration with Other Libraries: Beautiful Soup can be easily integrated with python libraries and frameworks, such as Requests for fetching web pages , Pandas for data manipulation , or Matplotlib for data visualization.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e160c7-0659-43e4-a437-723363e0a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qus 4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "057c7633-b797-46d0-84a1-27eefb290a97",
   "metadata": {},
   "source": [
    "Flask is commonly used in web scraping projects for several reasons:\n",
    "    1. Web Interface: Flask allows you to create a web interface for your web scraping application. you can build a simple user interface where users can input URLs or search queries , initiate the scraing process, and view the extracted data in a browser.\n",
    "    2. API Development : Flask can be used to create a RESTful API that exposes the scraping functionality.\n",
    "    3. Asynchrounous Processing : Web scraping often involves making multiple HTTP requests and processing large amounts of data , which can be tiem-consuming.\n",
    "    4. Customization : Flask provides a ligthweight framework that allows for easy customization and extension.\n",
    "    5. Integration with Frontend Frameworks: if you want to build a more sophisticatesd frontend for your scraping tool using modern JavaScript like React or Vue.js, Flask can serve as a backend API for fetching data and handling user requests.\n",
    "    6. Deployment : Flask applications can be easily deployed to various hosting platforms , including cloud services like Heroku, AWS , or Google Cloud Platform ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe01481a-e574-431f-8355-ca5fa99a928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qus 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcb75ad-c91b-487f-b848-7871b4692909",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1996971667.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1. Amazon EC2 (Elastic Compute Cloud)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1. Amazon EC2 (Elastic Compute Cloud):\n",
    "    Use: EC2 instances could be used to host the web scraping application.These instances provide scalable compute capacity inthe cloud , allowing you to run the scraping scripts and serve web interfaces or APIs.\n",
    "2. Amazon S3 (Simple Storage Service):\n",
    "    Use:S3 could be used to store the scraped data. After scraping , the data can be saved in S3 buckets , which provide highly durable and scalable object storage.\n",
    "    \n",
    "3. Amazon RDS (Relational Database Service):\n",
    "    Use: RDS could be to store metdata related to the scraped data or to maintain relational databases for storing information.\n",
    "4. Amozan SQS (Simple Queue Service):\n",
    "    Use: SQS could be used to manage the scraping tasks asynchronously. as scraping tasks are submitted by users or triggered by scheduled jobs , they can be added to an SQS queue,which decouples the components of the application . \n",
    "5. Amazon CloudWatch:\n",
    "    Use:CloudWatch could be used for monitoring and logging the performance of the scraping application.\n",
    "6. Amozan ECS (Elastic Container Services) or EKS (Elastic Kubernate Service):\n",
    "    Use:If you're using containerized applications , ECS or EKS can be used to orchestrate and manage the containers running the scraping application.\n",
    "7. AWS Lambda:\n",
    "    Use: Lambda could be used for serverless execution for certain components of the scraping application.\n",
    "8.Amazon API Gateway:\n",
    "    Use: API Gateway cou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925334d2-67db-4970-9bd7-f2e53b88cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    ":\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
